#!/usr/bin/perl

use warnings;
use strict;
use Getopt::Long;
use File::Basename;
use File::Path qw(make_path remove_tree);
use List::Util qw(sum all any min max); 
use List::MoreUtils qw(pairwise);
use Algorithm::Loops qw(NestedLoops);
use POSIX;
use LWP::Simple;
use Parallel::ForkManager;
use Sys::Hostname;
use Time::HiRes qw(time);

####Dev####
use Data::Dumper;
use Storable;
use File::Slurp qw(read_file write_file);

my $version = "0.6.2"; #22/08/17

my $help;
my $outpath;
my %parameters;
our $debug;
my $doblast;
my %blastparams =(
	blasthits => 25,
	assignsens => 0.4
);
my $maxmapseqs = 2000000;
my $mode;
our $threads = 1;
our $verbose;

my $script = basename($0,());

########################################################
# USAGE
#
my $usage =<<USAGE;

Description:

	This is $script version $version of the NGS Amplicon Pipeline. It carries out either barcode selection or clustering of one or several fastas. When selecting barcodes, there are three modes: viewing statistics for a single file, running one or several files in batch mode with a set of parameters, or iterating through range(s) of parameter(s) to explore parameter space. Clustering only has the latter two modes.
	
	For more details, see the google doc documentation.
	
USAGE
#
######################################################

GetOptions("output=s"		=> \$outpath,
	   "mode=s"		=> \$mode,
	   "length_var=s"	=> \$parameters{length_var},
	   "cluster_method=s"	=> \$parameters{cluster_method},
	   "filter_chimeras=s"	=> \$parameters{filter_chimeras},
	   "usearch_version=s"	=> \$parameters{usearch_version},
	   "denoise=s"		=> \$parameters{denoise},
	   "eemax=s"		=> \$parameters{eemax},
	   "cpv=s"		=> \$parameters{cpv},
	   "minsize=s"		=> \$parameters{minsize},
	   "seqlength=i"	=> \$parameters{seqlength},
	   "blastpath=s"	=> \$blastparams{blastpath},
	   "blasthits=s"	=> \$blastparams{blasthits},
	   "assignsens=s"	=> \$blastparams{assignsens},
	   "maxmapseqs=i"	=> \$maxmapseqs,
	   "doblast"		=> \$doblast,
	   "threads=i"		=> \$threads,
	   "verbose"		=> \$verbose,
	   "debug"		=> \$debug,
	   "help"		=> \$help) or die "Error: Incorrect options, try \"$script --help\" for more information.\n";

#
# Check and do parsing of inputs
#
print "$usage\n" and exit if($help);

die "I need some files to work on, see \"$script --help\" for more information\n" if(scalar @ARGV == 0);
die "Error: sequence length must be specified\n" unless(defined $parameters{seqlength});

my $pathroot = "$outpath/temp/";

if(!defined $outpath){
	die "Error: output directory must be specified, see \"$script --help\" for more information\n";
} else {
	print "Creating output directory if needed\n" if $verbose;
	$outpath =~ s/\/$//;
	$pathroot = "$outpath/temp/";
	make_path($pathroot);
}

$verbose = 1 if $debug;

my @files=@ARGV;


#
# Check for blast database and arrange threads
#

if(!$blastparams{blastpath} and $doblast){
	my $host = hostname;
	my %paths = (
		'ctag' => '/dh/blastdb/nt',
		'hpc-watson' => '/db/blastdb/nt'
	);
	if(exists($paths{$host})){
		$blastparams{blastpath} = $paths{$host};
	} else {
		die "Error: no path to BLAST database found, please give me one using --blastpath <path>\n";
	}
};
$blastparams{blastthreads} = $threads;


# Set defaults
$mode = "batch" unless(defined $mode);
my %paramdefaults = (
	'usearch_version'	=> 80,
	'filter_chimeras'	=> 'no',
	'denoise'		=> 'yes',
	'eemax'			=> 'NA',
	'minsize'		=> 2,
	'cluster_method'	=> 'usearch',
	'cpv'			=> 3,
	'assigned_cpv'		=> 3,
	'length_var'		=> 1,
);

my @params = keys %paramdefaults;
@parameters{@params} = map { defined $parameters{$_} ? $parameters{$_} : $paramdefaults{$_} } (@params);

@params = sort keys %parameters;

if($parameters{cluster_method} eq 'both'){
	$parameters{cluster_method} = "usearch,swarm";
}elsif($parameters{cluster_method} ne 'usearch' and $parameters{cluster_method} ne 'swarm'){
	die "Error: cluster method \'$parameters{cluster_method}\' not recognised (perhaps you meant \"both\"), see \"$script --help\" for more information\n";
}

if($parameters{filter_chimeras} eq 'both'){
	$parameters{filter_chimeras} = "yes,no";
	warn "\nWarning: cluster_method \"swarm\" does no chimera filtering\n" if $parameters{cluster_method} =~ /swarm/;
}elsif($parameters{filter_chimeras} eq 'no'){
	warn "\nWarning: cluster' method \"swarm\" does no chimera filtering, it is advisable to use a different option\n\n" if $parameters{cluster_method} =~ /swarm/;
}elsif($parameters{filter_chimeras} ne 'swarm' and $parameters{filter_chimeras} ne 'yes'){
	die "Error: value $parameters{filter_chimeras} passed to --filter_chimeras not recognised\n";
}

if($parameters{denoise} eq 'both'){
	$parameters{denoise} = "yes,no";
}elsif($parameters{denoise} ne 'no' and $parameters{denoise} ne 'yes'){
	die "Error: value $parameters{denoise} passed to --denoise not recognised\n";
}

$parameters{seqconv} = $parameters{seqlength}/100;

#
# Set up parameter iterations
#
my ($iterations,$expand_parameters,$n_discarded) = parse_parameters($mode,\%parameters);
my @n_iter = sort keys %$iterations;

#print "Debug: parsed iteration hash follows:" if $debug;
#print Dumper \$iterations if $debug;

printf "%d iterations successfully parsed%s\n\n",$#n_iter+1,$n_discarded ? ", after $n_discarded iterations removed due to redundant cpv assignments" : "" if($verbose and $#n_iter>0);

my %otusets;
my %stats = (
	general	=> ['iteration'],
	filcat	=> ['files_in','total_unfiltered_readsin','total_reads_qualityfiltered','total_reads_qualitypassed','total_reads_concatenated'],
	derep	=> ['sequence_length_variation_pc','minlength','maxlength','length_discarded','sequences_remain','n_groups','mean_groupsize','min_groupsize','max_groupsize'],
	size	=> ['n_groups_discarded_<minsize','n_groups_discarded_unoise','n_chimeras_unoise','n_groups_remain'],
	cluster	=> ['n_otus','n_chimeras_uchime','n_chimeras_cluster']
);



#
# Prep for pipeline
#

my %samples;
foreach my $file (@files){
	my ($dir,$name,$suffix) = fileparse($file);
	$samples{$name}=0;
}

#
# Quality filtering and concatenation
#
my %filcat_log;

my $fcpm = Parallel::ForkManager->new($threads);
my $fcpm_done = 0;

$fcpm->run_on_finish( sub {
	my ($pid, $exit_code, $ident, $exit_signal, $core_dump, $data_structure_reference) = @_;
	$fcpm_done++;
	printf "Completed $fcpm_done/%d%s concatenation iterations\n", scalar @{${$expand_parameters}{eemax}},$parameters{eemax} ne "NA" ? " quality filtering and" : "" if $verbose and $mode eq 'eval';
	die "Error: filtering and concatenation thread failed to output\n" unless $data_structure_reference;
	my $eemax = $data_structure_reference->{eemax};
	$filcat_log{$eemax} = $data_structure_reference->{fclog};
});

printf "Starting %s concatenation\n", $parameters{eemax} ne "NA" ? "quality filtering and" : "" if $verbose and $mode eq "batch";
printf "Starting %d%s concatenation iterations using $threads threads \n", scalar @{${$expand_parameters}{eemax}},$parameters{eemax} ne "NA" ? " quality filtering and" : "" if $verbose and $mode eq 'eval';

foreach my $eemax (@{${$expand_parameters}{eemax}}){
	my $pid = $fcpm->start and next;
	
	my ($fclog) = filter_concat(\@files,$pathroot,$eemax,\%stats);
	
	$fcpm->finish(0, {fclog => $fclog, eemax => $eemax});
}

$fcpm->wait_all_children;
print "\n";

#
# Do dereplication in serial
#

my %allfasta;
my %derep_log;
my $derep_it;
foreach my $eemax (@{${$expand_parameters}{eemax}}){
	printf "Debug: entered derep loop for eemax $eemax out of %s, loading hash ${pathroot}raw_e$eemax.hash\n", join(',',@{${$expand_parameters}{eemax}}) if $debug;
	my $raw = retrieve("${pathroot}raw_e$eemax.hash");
	foreach my $lvar (@{${$expand_parameters}{length_var}}){
		printf "\tDebug: entered derep loop for length var $lvar out of %s within eemax loop $eemax, starting derep subroutine on loaded hash with %d sequences\n", join(',',@{${$expand_parameters}{length_var}}), scalar keys %$raw if $debug;
		$derep_it++;
		($allfasta{$eemax}{$lvar},$derep_log{$eemax}{$lvar}) = derep($raw,$eemax,$lvar,$derep_it,$parameters{seqlength},\%stats);
		printf "\tDebug: completed derep loop for length var $lvar out of %s within eemax loop $eemax, completed derep subroutine on loaded hash with %d sequences, retrieved %d dereplicated sequences\n", join(',',@{${$expand_parameters}{length_var}}), scalar keys %$raw, scalar keys %{$allfasta{$eemax}{$lvar}} if $debug;
	}
	undef $raw;
	printf "Debug: completed derep loop for eemax $eemax, %s\n", $raw ? "hash unloading failed" :" hash correctly unloaded from memory" if $debug;
	
}

$pathroot = $pathroot."cluster.";

#
# Do denoising and size sorting
#


my %sort_log;

my $bpm = Parallel::ForkManager->new($threads);
my $bpm_done=0;

my $n_size_denoise_its = (scalar @{${$expand_parameters}{denoise}} * scalar @{${$expand_parameters}{length_var}} * scalar @{${$expand_parameters}{minsize}} * scalar @{${$expand_parameters}{eemax}});

$bpm->run_on_finish( sub {
	my ($pid, $exit_code, $ident, $exit_signal, $core_dump, $data_structure_reference) = @_;
	$bpm_done++;
	print "Completed $bpm_done/$n_size_denoise_its size sorting and denoising iterations\n" if $verbose and $mode eq 'eval';
	die "Error: size thread failed to output\n" unless $data_structure_reference;
	my $eemax = $data_structure_reference->{eemax};
	my $dns = $data_structure_reference->{dns};
	my $lvar = $data_structure_reference->{lvar};
	my $mins = $data_structure_reference->{mins};
	$sort_log{$eemax}{$dns}{$lvar}{$mins} = $data_structure_reference->{slog};
});

print "\nStarting size sorting and denoising\n\n" if $verbose and $mode eq "batch";
print "\nStarting $n_size_denoise_its size sorting and denoising iterations using $threads threads\nc\n" if $verbose and $mode eq "eval";
foreach my $eemax (@{${$expand_parameters}{eemax}}){
	foreach my $lvar (@{${$expand_parameters}{length_var}}){
		foreach my $dns (@{${$expand_parameters}{denoise}}){
			foreach my $mins (@{${$expand_parameters}{minsize}}){
				my $pid = $bpm->start and next;
				
				my ($slog) = sort_write_denoise($allfasta{$eemax}{$lvar},$eemax,$pathroot,$dns,$lvar,$mins,\%stats);
				
				$bpm->finish(0, {slog => $slog, dns => $dns, lvar => $lvar, mins => $mins, eemax => $eemax});
			}
		}
	}
}

$bpm->wait_all_children;
print "\n";

#
# Do clustering and mapping pipeline
#
my %cluster_log;

my $pm = Parallel::ForkManager->new($threads);
my $done_iterations=0;

$pm->run_on_finish( sub {
	my ($pid, $exit_code, $ident, $exit_signal, $core_dump, $data_structure_reference) = @_;
	$done_iterations++;
	printf "Completed $done_iterations/%d total iterations\r",($#n_iter+1) if $verbose and $mode eq 'eval';
	die "Error: iteration failed to output\n" unless $data_structure_reference;
	my $iter = $data_structure_reference->{iter};
	$cluster_log{$iter} = $data_structure_reference->{log};
	$otusets{$iter} = $data_structure_reference->{otus};
});

print "Starting clustering\n" if $verbose and $mode eq "batch";
printf "Starting %d clustering iterations using $threads threads \n", ($#n_iter+1) if $verbose and $mode eq "eval";

foreach my $iter (keys %$iterations){;
	my $pid = $pm->start and next;
	
	my ($log,$otus) = cluster_and_map($pathroot,$iterations,$iter,\@params,\%stats,$maxmapseqs,\%samples);
	
	$pm->finish(0, {log => $log, otus => $otus, iter => $iter});
}

$pm->wait_all_children;
print "\n";

#
# Process otus
#
if($doblast){
	print "BLASTing OTUs across all iterations and making best-guess taxonomies\n";

	my %allotuseqs;
	foreach my $iter (keys %otusets){
		foreach my $seq (keys %{$otusets{$iter}}){
			push(@{$allotuseqs{$seq}},"$iter,$otusets{$iter}{$seq}");
		}
	}

	my %seqstoblast;
	@seqstoblast{1..scalar keys %allotuseqs} = keys %allotuseqs;

	my $taxonomies = batch_taxonomyblast(\%seqstoblast,\%blastparams);
#	print Dumper $taxonomies;
	my $taxonomypath = "$outpath/cluster.otus.taxonomy.csv";
	open my $taxout, '>', $taxonomypath;
	print $taxout "iteration,otu_id,duplicate,taxonomy_lineage,nhits,n_contributing_lineages,prop_contributing_lineages,top_contributor_id,mean_contributor_id,mean_contributor_score\n";
	foreach my $id (keys %seqstoblast){
		my @duplicates = @{$allotuseqs{$seqstoblast{$id}}};
		foreach my $i (0..$#duplicates){
			print $taxout "$duplicates[$i],$i,",join(";",@{${$taxonomies}{$id}{bgt}}),",${$taxonomies}{$id}{nhits},${$taxonomies}{$id}{nmatch},${$taxonomies}{$id}{prop},";
			print $taxout "${$taxonomies}{$id}{topid},${$taxonomies}{$id}{id},${$taxonomies}{$id}{score}\n";
		}
	}
	close $taxout;
	print "Completed, taxonomies written to $taxonomypath\n";
}
#
# Output results log
#
my $logpath = "$outpath/cluster.$mode.log.csv";
open my $logcsv, '>',$logpath ;

my @head = (@{$stats{general}},@params,@{$stats{filcat}},@{$stats{derep}},@{$stats{size}},@{$stats{cluster}});


print Dumper \%stats if $debug;
print Dumper \@params if $debug;
print Dumper \%cluster_log if $debug;
print Dumper \%filcat_log if $debug;
print Dumper \%derep_log if $debug;
print Dumper \%sort_log if $debug;

print $logcsv join(',',@head);

foreach my $i (keys %$iterations){
	my $eemax = ${$iterations}{$i}{eemax};
	my $dns = ${$iterations}{$i}{denoise};
	my $lvar = ${$iterations}{$i}{length_var};
	my $mins = ${$iterations}{$i}{minsize};
	print $logcsv "\n$i,",
		join(',',@{$cluster_log{$i}}{@params},
			@{$filcat_log{$eemax}}{@{$stats{filcat}}},
			@{$derep_log{$eemax}{$lvar}}{@{$stats{derep}}},
			@{$sort_log{$eemax}{$dns}{$lvar}{$mins}}{@{$stats{size}}},
			@{$cluster_log{$i}}{@{$stats{cluster}}}
			);
}

close $logcsv;

print "\nCleaning temporary files\n" if $verbose and !$debug;
system "rm -rd $outpath/temp" unless $debug;

print "\nCSV log successfully written to $logpath\n" if $verbose;

exit;

sub filter_concat{
	my ($files,$pathroot,$eemax,$stats) = @_;
	print timestamp(),": starting concatenation iteration with",$eemax ne "NA" ? " filtering at eemax $eemax":"out filtering",", pid $$\n", if $verbose;
	
	my $filterpath = $pathroot."filter_e$eemax/";
	make_path($filterpath);
	
	#
	# Quality filter files
	#
	my $files_in = scalar @files;
	my ($total_readsin,$total_readsdiscarded,$total_readspassed);
	my @filteredfiles;
	if($eemax ne "NA"){
		my $nfiles = scalar @files;
		open my $fillog, '>', "${outpath}/filter_log_e$eemax.csv";
		print $fillog "file,reads,discarded,passed\n";
		my $filesdone;
		foreach my $file (@$files){
			$filesdone++;
			print timestamp(),": filtering $filesdone/$nfiles files, pid $$\n" if $verbose;
			my ($name,$dir,$suffix) = fileparse($file);
			$name =~ s/^(.+)\.[^.]+$/$1/;
			my $filterfile = "$filterpath$name.fa";
			push @filteredfiles, $filterfile;
			my $filter_cmd = "usearch92 -fastq_filter $file -fastaout $filterfile -fastq_maxee $eemax 2>&1 1>/dev/null";
			#print "Filter command:\n$filter_cmd\n" if $debug;
			
			my $filter_log=`$filter_cmd`; #Filter using USEARCH70 fastq_filter, saving terminal output. Note that this function outputs USEARCH70 version information to STDOUT and the actual useful information about filtering to STDERR, hence the redirect of STDERR to STDOUT and discarding of STDOUT
			$filter_log =~ s/\r/\n/g;
			#print "Filter log:\n$filter_log\n" if $debug;
			
			if($filter_log=~ /.*\s(\d+)\s+Reads.+\s(\d+)\s+Discarded.+\s(\d+)\s+Filtered/s){ #regex to extract data
				print $fillog "$name,$1,$2,$3\n";
				$total_readsin += $1;
				$total_readsdiscarded += $2;
				$total_readspassed += $3;
			} elsif($filter_log=~ /Fatal\serror/) {
				die "Filtering failed for file $file, are you sure it's a FASTQ file? Do not set eemax if running with FASTA files\n" if($filter_log =~ /errno=34/);
				die "Filtering failed for file $file\n";
			}
		}
		close $fillog;
	} else {
		@filteredfiles = @$files;
		($total_readsin,$total_readsdiscarded,$total_readspassed) = ("NA" x 3);
	}
	
	#
	# Read in files then output raw
	#
	print timestamp(),": reading and concatenating files, pid $$\n" if $verbose;
	my %allfasta;
	foreach my $file (@filteredfiles){
		my %fasta = read_fasta($file,"check","order");
	#	printf "$file has %d sequences\n", scalar keys %fasta if $debug;
		warn "Warning: file $file is empty!\n" if(scalar keys %fasta < 1);
		my ($name,$dir,$suffix) =fileparse($file);
		$name =~ s/^(.+)\.[^.]+$/$1/;
		@allfasta{keys %fasta} = values %fasta;
	}
	
	my $concat_reads = scalar keys %allfasta;
	
	print timestamp(),": concatenation of $files_in files has $concat_reads sequences\n\n" if $verbose;
	die "Error, no sequences in any input fasta" if(scalar keys %allfasta<1);

	my $allreads_raw = "$outpath/cluster.allreads_raw_e$eemax.fa";
	open my $allraw, '>', $allreads_raw;
	foreach my $id (keys %allfasta){
		print $allraw ">$id\n$allfasta{$id}{seq}\n";
	}
	close $allraw;
	store \%allfasta, "${pathroot}raw_e$eemax.hash";
	print "Written concatenated reads fasta to $allreads_raw and stored in in ${pathroot}raw_e$eemax.hash\n" if $debug;
	undef %allfasta;
	
	#
	# Do logging
	#
	
	my %log;
	@log{@{${$stats}{filcat}}} = ($files_in,$total_readsin,$total_readsdiscarded,$total_readspassed,$concat_reads);
	return \%log;
}

sub derep{
	my ($raw,$eemax,$lvar,$derep_it,$len,$stats) = @_;
	my $minl = floor($len-($lvar*$len)/100);
	my $maxl = ceil($len+($lvar*$len)/100);
	
	print timestamp(),": starting serial dereplication iteration $derep_it with length_var $lvar%: $minl-${maxl}bp inclusive\n", if $verbose;
	my %dlog;
	printf "Debug: input hash has %d sequences\n",scalar keys %$raw if $debug;
	#
	# Dereplicate all sequences within length variation
	#
	my $length_discarded = 0;
	my $length_passed = 0;
	my %dereplicated;
	foreach my $id (keys %$raw){
		if(length(${$raw}{$id}{seq})>=$minl and length(${$raw}{$id}{seq})<=$maxl){
			$length_passed++;
			if(!defined $dereplicated{${$raw}{$id}{seq}}{order} or $dereplicated{${$raw}{$id}{seq}}{order} > ${$raw}{$id}{order}){
				$dereplicated{${$raw}{$id}{seq}}{order} = ${$raw}{$id}{order};
			}
			$dereplicated{${$raw}{$id}{seq}}{count}++;
		} else {
			$length_discarded++;
		}
	}
	my $ngroup = scalar keys %dereplicated;
	
	print timestamp(),": discarded $length_discarded sequences <${minl}bp or >${maxl}bp, $length_passed sequences dereplicated into $ngroup unique sequences\n" if $verbose; 	
	#
	# Calculate statistics
	#
	my @groupsizes = map { $dereplicated{$_}{count} } keys %dereplicated;
	my %sizecount;
	foreach my $size (@groupsizes){
		$sizecount{$size}++;
	}
	
	my $singletons  = $sizecount{1} ? $sizecount{1} : 0;
	my $minsize = min(@groupsizes);
	my $topsize = max(@groupsizes);
	my $mean_groupsize = $length_passed/$ngroup;
	
	@dlog{@{${$stats}{derep}}} = ($lvar,$minl,$maxl,$length_discarded,$length_passed,$ngroup,$mean_groupsize,$minsize,$topsize);
	
	return (\%dereplicated,\%dlog);
}

sub sort_write_denoise{
	my($dereplicated,$eemax,$pathroot,$dns,$lvar,$mins,$stats) = @_;
	
	my %slog;
	
	my $printstring = join "", $threads>1 ? " pid $$" : "" , " eemax $eemax denoise $dns length_var $lvar minsize $mins";
	
	my $sortpath = $pathroot."sort_eemax$eemax\_dns$dns\_lvar$lvar\_mins$mins.fa";
	my $denoisepath = $pathroot."denoised_eemax$eemax\_dns$dns\_lvar$lvar\_mins$mins.fa";
	
	my $ngroup = scalar keys %$dereplicated;
	
	printf timestamp(), " : starting size sorting iteration with min size $mins%s, pid $$\n", $eemax eq "NA" ? " without denoising" : "with denoising", if $verbose;
	
	#
	# Output sequences size sorted
	#
	my @sorted_seqs = sort { ${$dereplicated}{$b}{count} <=> ${$dereplicated}{$a}{count} or 
				 ${$dereplicated}{$a}{order} <=> ${$dereplicated}{$b}{order}
				} keys %$dereplicated;
	open my $sorted, '>', $sortpath;
	my $bigenough = 0;
	foreach my $seq (@sorted_seqs){
		if(${$dereplicated}{$seq}{count}>=$mins){
			$bigenough++;
			print $sorted ">sequence$bigenough;size=${$dereplicated}{$seq}{count};order=${$dereplicated}{$seq}{order}\n$seq\n";
		} else {
			last;
		}
	}
	
	my $n_groups_remain = $bigenough;
	my $minsize_discarded = $ngroup - $n_groups_remain;
	
	print timestamp(),":$printstring discarded $minsize_discarded groups, $n_groups_remain remain, written to $sortpath\n" if $verbose;
	
	
	#
	# Perform denoising
	#
	
	my $nseq_discarded_unoise="NA";
	my $n_chimeras_unoise="NA";
	
	if($dns eq "yes" ){
		print timestamp(),":$printstring performing unoise\n" if $verbose;
		
		my $denoise_cmd = "usearch92 -unoise2 $sortpath -fastaout $denoisepath 2>&1";
		my $denoise_log = `$denoise_cmd`;
		$denoise_log =~ s/\r/\n/g;
		print "\nDenoise command:\n$denoise_cmd\nDenoise log:\n$denoise_log\n" if $debug;
		
		$denoise_log =~ /.*\s(\d+)\samplicons,\s(\d+)\sbad.*\s(\d+)\sgood,\s(\d+)\schimeras/s;
		print "Parsed |$1,$2,$3,$4|\n" if $debug;
		$nseq_discarded_unoise = $n_groups_remain-$3;
		$n_chimeras_unoise = $4;
		$n_groups_remain = $3;
		
		print timestamp(),":$printstring completed denoising, $n_groups_remain unique sequences remain, written to $denoisepath\n" if $verbose;
	}
	
	@slog{@{${$stats}{size}}} = ($minsize_discarded,$nseq_discarded_unoise,$n_chimeras_unoise,$n_groups_remain);
	
	return(\%slog);
}


sub cluster_and_map{
	my ($pathroot,$iterations,$iter,$params,$stats,$maxmapseqs,$samples) = @_;
	
	my %log;
	
	printf "%s: starting iteration $iter of %d with pid $$\n",timestamp(),scalar keys %$iterations if $verbose;
	my $printstring = join "", ($threads>1 and scalar keys %$iterations>1) ? " pid $$" : "" , scalar keys %$iterations>1 ? " iteration $iter" : "";
	
	#print Dumper ${$iterations}{$iter};
	
	#
	# Prepare paths
	#
	
	# Existing paths for source of this iteration
	my $sortpath = $pathroot."sort_eemax${$iterations}{$iter}{eemax}_dns${$iterations}{$iter}{denoise}_lvar${$iterations}{$iter}{length_var}_mins${$iterations}{$iter}{minsize}.fa";
	my $denoisepath = $pathroot."denoised_eemax${$iterations}{$iter}{eemax}_dns${$iterations}{$iter}{denoise}_lvar${$iterations}{$iter}{length_var}_mins${$iterations}{$iter}{minsize}.fa";
	
	# New paths
	$pathroot .= "iteration$iter.";
	my $chimerapath = $pathroot."chimeras.fa";
	my $nonchimerapath = $pathroot."nonchimeras.fa";
	my $otuspath = $pathroot."otus.fa";
	my $ucpath = $pathroot."map.uc";
	my $mappath = $pathroot."map.csv";
	my $swarmpath = $pathroot."swarmotus.csv";
	
	my $chimerainput = $denoisepath;
	$chimerainput = $sortpath if(${$iterations}{$iter}{denoise} eq "no");
	
	#
	# Perform chimera filtering
	#
	
	my $n_chimeras_uchime = "NA";
	my $clusterinput = $chimerainput;
	if(${$iterations}{$iter}{filter_chimeras} eq "yes" or ${$iterations}{$iter}{filter_chimeras} eq "swarm"){
		if(${$iterations}{$iter}{denoise} eq "yes"){
			print timestamp(),":$printstring filtering chimeras already performed by denoising, skipping\n" if $verbose;
		} else {
			print timestamp(),":$printstring filtering chimeras\n" if $verbose;
			
			my $chimera_cmd = "usearch92 -uchime2_denovo $chimerainput -chimeras $chimerapath -nonchimeras $nonchimerapath 2>&1";
			print "Command: $chimera_cmd\n" if $debug;
			my $chimera_log = `$chimera_cmd`;
			$chimera_log =~ s/\r/\n/g ;
			print "Log:\n$chimera_log\n" if $debug;
			$chimera_log =~ /.*,\s(\d+)\schimeras/;
			$n_chimeras_uchime = $1;
			$clusterinput = $nonchimerapath;
			
			print timestamp(),":$printstring completed filtering $n_chimeras_uchime chimeras, written to $nonchimerapath\n" if $verbose;
		}
	}
	
	my ($nOTUs,$n_chimeras_cluster);
	if (${$iterations}{$iter}{cpv} == 0){
		print timestamp(),":$printstring clustering skipped because cpv is 0\n" if $verbose;
		system("cp $clusterinput $otuspath");
		my $seqs = `cat $clusterinput | grep -c '^>'`;
		($nOTUs,$n_chimeras_cluster) = ($seqs-{${$iterations}{$iter}{filter_chimeras} eq "yes"  ? 0 : $n_chimeras_uchime},"NA");
	} else {
		print timestamp(),":$printstring starting clustering using ${$iterations}{$iter}{cluster_method} assignment value ${$iterations}{$iter}{assigned_cpv} on file $clusterinput\n" if $verbose;
		if(${$iterations}{$iter}{cluster_method} eq "usearch"){
		
			my $cluster_cmd = "usearch${$iterations}{$iter}{usearch_version} -cluster_otus $clusterinput -otus $otuspath -otu_radius_pct ${$iterations}{$iter}{cpv} -relabel otu 2>&1";
			print "Command: $cluster_cmd\n" if $debug;
			my $cluster_log = `$cluster_cmd`;
	#		print $fh $cluster_log,"\n";
			print "Log:\n$cluster_log\n" if $debug;
			if($cluster_log =~ /.*\s(\d+)\sOTUs,\s(\d+)\schimeras/s){#.* at beginning of regex ensures last OTU and chimera values selected (the function prints multiple lines of progress using \r!)
				($nOTUs,$n_chimeras_cluster) = ($1,$2);
			} elsif( $cluster_log =~ /OTUs\s\s(\d+).+Chimeras\s\s(\d+)/s){
				($nOTUs,$n_chimeras_cluster) = ($1,$2);
			}
	#		print "cluster chimeras: $2\n";
			
		} elsif(${$iterations}{$iter}{cluster_method} = "swarm"){
		
			my @bases = ('A','C','T','G');
			my $randbase = $bases[int(rand(4))];
			my $sed_cmd = "sed -i -e'/^>/ ! s/N/$randbase/g' $clusterinput";
	#		print $sed_cmd,"\n";
			system($sed_cmd);
			my $cluster_cmd = "swarm -z -t 1 -d ${$iterations}{$iter}{assigned_cpv} -o $swarmpath -w $otuspath $clusterinput 2>&1";
	#		print $cluster_cmd,"\n";
			my $cluster_log = `$cluster_cmd`;
	#		print $cluster_log,"\n";
			$cluster_log =~ /Number\sof\sswarms:\s+(\d+)/;
			($nOTUs,$n_chimeras_cluster) = ($1,"NA");
		}
	}
#	close $fh;
	print timestamp(),":$printstring completed clustering, $nOTUs OTUs written to $otuspath\n" if $verbose;
	
	#
	# Read OTUs to hash for output
	#
	
	my %otus = read_fasta($otuspath);
	my %otus_rev = map { $otus{$_} => $_ } keys %otus;
	
	#
	# Map OTUs
	#
	
	my $allreads_raw = "$outpath/cluster.allreads_raw_e${$iterations}{$iter}{eemax}.fa";
	
	my $idpc;
	if(${$iterations}{$iter}{cpv} == 0){
		$idpc=0.999;
	} else {
		$idpc=1-(${$iterations}{$iter}{cpv}/100);
	}
	print timestamp(),":$printstring starting mapping all reads to OTUs using id $idpc\n" if $verbose;
	my $mapcmd = "usearch80 -usearch_global $allreads_raw -db $otuspath -strand plus -id $idpc -uc $ucpath 2>&1";
	print "Command: $mapcmd\n";
	my $maplog=`$mapcmd`;
	print "Log:\n$maplog\n";
	if($maplog =~ /File\ssize\stoo\sbig/s){
		undef $maplog;
		unlink $ucpath;
		print timestamp(),":$printstring initial mapping failed, likely for too many sequences, attempting iterative mapping mode\n" if $verbose;
		my $ninseqs = `cat $allreads_raw | grep -c '^>'`;
		my $map_iterations = ceil($ninseqs/$maxmapseqs);
		
		#print timestamp(),":$printstring maplication loop $loop on file $allreads_raw with $ninseqs sequences, concatenating to $map_loop_out over $map_iterations iterations\n" if $verbose;
		foreach my $it (1..$map_iterations){
			my $linemin = 2*$maxmapseqs*($it-1)+1;
			my $linemax = 2*$maxmapseqs*$it;
			my $tempin = $pathroot."mapitersubin_$it.fa";
			my $tempout = $pathroot."mapitersubout_$it.uc";
			print timestamp(),":$printstring starting mapping iteration $it using lines $linemin to $linemax of $allreads_raw in temporary file $tempin\n" if $verbose;
			my $subcmd = "sed -n $linemin,$linemax"."p $allreads_raw > $tempin";
			system("$subcmd");
			my $map_cmd = "usearch80 -usearch_global $tempin -db $otuspath -strand plus -id $idpc -uc $tempout 2>&1";
			print "Command: $map_cmd\n" if $debug;
			my $maplog = `$map_cmd`;
			print "Log:\n$maplog\n" if $debug;
			printf "%s\n", (split("\n",$maplog))[7];
			system("cat $tempout >> $ucpath");
			system("rm $tempin $tempout") unless $debug;
			print timestamp(),":$printstring completed iteration $it, $tempout concatenated to $ucpath and temporary files deleted\n";
		}
		print timestamp(),":$printstring completed $map_iterations iterations, results concatenated to $ucpath\n" if $verbose;
	}
	
	convert_uc_and_write($ucpath,$mappath,$samples);
	
	print timestamp(),":$printstring completed mapping, map written to $mappath\n" if $verbose;
	
	#
	# Final logging and output
	#
	system("mv $otuspath $outpath/otus_iteration$iter.fa");
	system("mv $mappath $outpath/map_iteration$iter.csv");
	
	@log{@$params} = @{${$iterations}{$iter}}{@$params};
	@log{@{${$stats}{cluster}}} = ($nOTUs,$n_chimeras_uchime,$n_chimeras_cluster);
	
	print timestamp(),":$printstring completed\n" if $verbose;
	
	return (\%log,\%otus_rev);
}

sub batch_taxonomyblast{
	my ($queryhash,$parameters) = @_;
	my $querypath = "tempblastfasta.fa";
	my @ids = keys %$queryhash;
	my %out;
	my $maxcalls = 200;
	# output fasta
	open my $ofa, '>', $querypath;
	foreach my $id (keys %$queryhash){
		print $ofa ">$id\n${$queryhash}{$id}\n";
	}
	close $ofa;
	
	# blast
	printf "%d sequences to BLAST, this %s\n",scalar @ids, scalar @ids/${$parameters}{blastthreads}>6 ? scalar @ids/${$parameters}{blastthreads}>12 ? "will take some time" : "may take some time" : "won't take too long";
	my $blastcmd = "blastn -query $querypath -db ${$parameters}{blastpath} -num_threads ${$parameters}{blastthreads} -max_target_seqs ${$parameters}{blasthits} -outfmt \"6 qseqid sacc bitscore pident\"";
	my $blastdump;
	my $blasttries = 0;
	while(!$blastdump && $blasttries<5){
		$blasttries++;
		$blastdump = qx(bash -c '$blastcmd');
	}
#	open my $bd, '>', "blastdump.txt";
#	print $bd $blastdump;
#	close $bd;
	if($blastdump){
		print "Retrieving taxonomy information\n";
		my ($hits,$accessions) = parse_blastdump($blastdump);
		my $eutil_calls = ceil(scalar @$accessions/$maxcalls);
		printf "%d hits to retrieve data for, this will take at least %d seconds\n",scalar @$accessions, $eutil_calls;
		foreach my $call (1..$eutil_calls){
			my $starttime = time;
			my $min = ($call-1)*$maxcalls;
			my $max = $call*$maxcalls-1;
			$max = $max>$#{$accessions} ? $#{$accessions} : $max;
			print "eutil call $call for hits $min to $max...";
			my $accessionstring = join ",",@{$accessions}[$min..$max];
			my $nucdump = get("https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=nuccore&id=$accessionstring&retmode=xml&rettype=fasta");
			my ($taxids,$taxidstring) = parse_nucdump($nucdump);
			my $taxdump = get("https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=taxonomy&id=$taxidstring");
			my $lineages = parse_taxdump($taxdump);
			$hits = merge_data($hits,$taxids,$lineages);
			my $duration = time - $starttime;
			print "took $duration seconds\n";
			sleep (0.7-$duration) if ($duration<0.7); #ensures the frequency of eutil requests is lower than 3 per second, as recommended by NCBI.
		}
		store $hits, "hithashstore.log" if $debug;
		print "Data retrieved, processing and making best guesses\n";
		foreach my $qid (@ids){
			if(!exists(${$hits}{$qid})){
				$out{$qid}{nhits} = 0;
				$out{$qid}{bgt} = ["NA"];
				$out{$qid}{nmatch} = 0;
				$out{$qid}{prop} = "NA";
				$out{$qid}{score} = "NA";
				$out{$qid}{id} = "NA";
				$out{$qid}{topid} = "NA";
				next;
			}
#			print "otu $qid ";
			my @linaoa;
			my @lin_acc;
			foreach my $acc (keys %{${$hits}{$qid}}){
				push @linaoa,[ split /;\s/,${$hits}{$qid}{$acc}{lineage} ];
				push @lin_acc, $acc;
				$out{$qid}{nhits}++;
			}
			
			my @matchi = ((1) x scalar @linaoa);
			my $totalhits = scalar @linaoa;
			my $maxlength = max( map { $#{$_} } @linaoa );
			my $outi = $maxlength;
			my $prop;
			my $nmatch;
			foreach my $ti (0..$maxlength){
				printf "run comparison for taxon $ti of %d lineages with maxlength of $maxlength taxa\n", scalar @linaoa if $debug;
				#print Dumper \@linaoa;
				my @testarray = map {$linaoa[$_][$ti]} (0..$#linaoa);
				{ no warnings 'uninitialized';print join(',',@testarray),"\n" if $debug;}
				@matchi = most_frequent_indices(@testarray);
				print join(',',@matchi),"\n";
				printf "comparison found %d identical...",sum(@matchi) if $debug;
				if(sum(@matchi)/$totalhits <= ${$parameters}{assignsens}){
					$outi = $ti-1;
					print "lineages too divergent, ending at taxon $outi\n" if $debug;
					last;
				}
				$nmatch = sum(@matchi);
				$prop = sum(@matchi)/$totalhits;
				print "sufficient similarity, subsetting..." if $debug;
				my @nextlinaoa;
				foreach my $i (0..$#linaoa){
					push @nextlinaoa,$linaoa[$i] if($linaoa[$i][$ti+1] and $matchi[$i]);
				}
				if(!@nextlinaoa){
					$outi = $ti;
					print "no further taxa for remaining lineages, ending at taxon $outi\n" if $debug;
					last;
				}
				$maxlength = max( map { $#{$_} } @nextlinaoa );
				printf "%d/$totalhits lineages remain for next comparison, new maxlength of $maxlength\n", scalar @nextlinaoa if $debug;
				@linaoa = @nextlinaoa;
			}
			
			my @ind = grep { $matchi[$_] == 1 } 0..$#matchi;
			my @acc_used = @lin_acc[@ind];
			$out{$qid}{bgt} = [ @{$linaoa[$ind[0]]}[0..$outi] ];
			$out{$qid}{nmatch} = $nmatch;
			$out{$qid}{prop} = $prop;
			my @scores = map {${$hits}{$qid}{$_}{score}} @acc_used;
			$out{$qid}{score} = sum(@scores)/scalar @scores;
			my @ids = map {${$hits}{$qid}{$_}{pident}} @acc_used;
			$out{$qid}{id} = sum(@ids)/scalar @ids;
			$out{$qid}{topid} = max(@ids);
		}
	}
	system("rm $querypath");
	print Dumper \%out if $debug;
	return \%out;
}

sub most_frequent_indices{
	my (@items) = @_;
	my %count;
	foreach my $item (@items){
		$count{$item}++ if($item);
	}
#	print Dumper \%count;
	my $max = max(values %count);
	my @mfis = map {!defined($_) ? 0 : $count{$_} == $max ? 1 : 0 } @items;
	return @mfis;
}

sub merge_data{
	my ($hits,$taxids,$lineages) = @_;
	foreach my $qid (keys %$hits){
		foreach my $acc (keys %{${$hits}{$qid}}){
			if(exists(${$taxids}{$acc})){
				my $tid = ${$taxids}{$acc};
				${$hits}{$qid}{$acc}{tid} = $tid;
				${$hits}{$qid}{$acc}{lineage} = ${$lineages}{$tid};
			}
		}
	}
	return $hits;
}

sub parse_blastdump{
	my ($blastdump) = @_;
	my %hits;
	open my $bd, '<', \$blastdump;
	my %acchash;
	while(my $row = <$bd>){
		chomp $row;
		my @line = split /\t/, $row;
		$hits{$line[0]}{$line[1]}{score} = $line[2];
		$hits{$line[0]}{$line[1]}{pident} = $line[3];
		$acchash{$line[1]} = 1;
	}
	close $bd;
	my @accessions = keys %acchash;
	return (\%hits,\@accessions);
}

sub parse_nucdump{
	my ($nucdump) = @_;
	my %tids;
	open my $nd, '<', \$nucdump;
	my @taxids;
	my ($acc,$tid);
	while(my $row = <$nd>){
		chomp $row;
#		print $row," - ";
		if($row eq "<TSeq>"){
			($acc,$tid) = (undef,undef);
#			print "start of section, reset\n";
		} elsif($row =~ /<TSeq_accver>([^.<]+)[^<]+<\/TSeq_accver>/){
			$acc = $1;
#			print "found accession $acc\n";
		} elsif($row =~ /<TSeq_taxid>(\d+)<\/TSeq_taxid>/){
			$tid = $1;
#			print "found tid $tid\n";
			push @taxids,$tid;
		} elsif($row eq "<\/TSeq>"){
			$tids{$acc} = $tid;
#			print "end of section, placed tid into hash\n"; 
#		} else {
#			print "not relevant\n";
		}
	}
	close $nd;
	my $taxidstring = join(',',@taxids);
	return (\%tids,$taxidstring);
}

sub parse_taxdump{
	my ($taxdump) = @_;
	my %lins;
	open my $td, '<', \$taxdump;
	my ($flag,$tid,$lineage, $species);
	while(my $row = <$td>){
		chomp $row;
#		print $row, " - ";
		if($row =~ /<Taxon>/){
			$flag++;
#			print "entering taxon of level $flag\n";
		} elsif($row =~ /<TaxId>(\d+)<\/TaxId>/ and $flag == 1){
			$tid = $1;
#			print "found taxid $tid\n"
		} elsif($row =~ /<Lineage>([^<]+)<\/Lineage>/ and $flag == 1){
			$lineage = $1;
#			print "found lineage $lineage\n";
		} elsif($row =~ /<ScientificName>([^<]+)<\/ScientificName>/ and $flag == 1){
			$species = $1;
#			print "found lineage $lineage\n";
		} elsif($row =~/<\/Taxon>/){
#			print "end of taxon level $flag";
			$flag--;
			if($flag == 0){
#				print "placing lineage into hash for tid and resetting\n";
				$lins{$tid} = $lineage;
				$lins{$tid} .= "; $species" if $species;
				($tid,$lineage,$species) = (undef,undef,undef);
#			} else {
#				print "not yet at low enough level to add to hash\n";
			}
#		} else {
#			print "not relevant\n";
		}
	}
	close $td;
	return \%lins;
}

sub timestamp{
my @time = localtime;
return sprintf "%02d:%02d:%02d",$time[2],$time[1],$time[0];
}

sub parse_parameters {
	my ($inmode, $inparams) = @_;
	my %param_expand;
	my @paramarray;
	my $n_singleparams;
	die "Input parameters not single values, did you mean to use \"--mode eval\"?\n" unless $inmode eq "eval" or all { $_ =~ /^[^,-]*$/ } values %$inparams;
	foreach my $param (keys %$inparams){
		push @paramarray,$param;
		if($inmode eq "batch"){
			$param_expand{$param} = [$inparams->{$param}];
		} elsif ($inmode eq "eval"){
			if ($inparams->{$param} =~ /^(\d+(\.\d+)?)-(\d+(\.\d+)?),(\d+(\.\d+)?)$/ ){
				my @values;
				my $curmax = $1;
				while ($curmax <= $3){
					push @values, $curmax;
					$curmax += $5;
				}
				$param_expand{$param} = \@values;
			} elsif($inparams->{$param} =~ /^[a-zA-Z\d]+(?:,[a-zA-Z\d]+)+$/){
				my @values = split ',',$inparams->{$param};
				$param_expand{$param} = \@values;
			} elsif($inparams->{$param} =~ /^[a-zA-Z]+$|^\d+(\.\d+)?$/){
				$param_expand{$param} = [$inparams->{$param}];
				$n_singleparams++;
				die "No parameter ranges given, did you mean to use \"--mode batch\"?\n" if($n_singleparams == scalar keys %$inparams);
			} else {
				die "Could not successfully parse string given for $param\n";
			}
		} else {
			die "Argument passed to --mode not recognised, see \"$script --help\" for more information\n"
		}
	}
	my @allarray;
	NestedLoops([@param_expand{@paramarray}], sub {push @allarray, [ @_ ]});
	#print Dumper \@allarray if $debug;
	my $nonint_removed;
	my %iterhash;
	my %hashcompare;
	foreach my $it (0 .. $#allarray){
		#printf "\nChecking iteration %d\n", $it+1 if $debug;
		$iterhash{$it+1} = {map { $paramarray[$_] => $allarray[$it][$_] } 0..$#paramarray};
		#print Dumper \$iterhash{$it+1} if $debug;
		if($iterhash{$it+1}{cluster_method} eq "swarm"){
			#print "Method is swarm\n" if $debug;
			$iterhash{$it+1}{usearch_version} = "NA";
			$iterhash{$it+1}{assigned_cpv} = sprintf( "%.0f", $iterhash{$it+1}{cpv}*$iterhash{$it+1}{seqconv});
			$iterhash{$it+1}{cpv} = $iterhash{$it+1}{assigned_cpv}/$iterhash{$it+1}{seqconv};
			my $compvalue = "$iterhash{$it+1}{cluster_method}-$iterhash{$it+1}{usearch_version}-$iterhash{$it+1}{assigned_cpv}";
			if(exists $hashcompare{$compvalue}){
				delete $iterhash{$it+1};
				$nonint_removed++;
			} else {
				$hashcompare{$compvalue} = 1;
			}
		} else {
			$iterhash{$it+1}{assigned_cpv} = $iterhash{$it+1}{cpv};
			$hashcompare{$iterhash{$it+1}{cluster_method}} = 1;
		}
	}
	
	my $newid = 1;
	my %newhash;
	foreach my $oldid (keys %iterhash){
		$newhash{$newid} = $iterhash{$oldid};
		$newid++
	}
	return (\%newhash,\%param_expand,$nonint_removed);
}

sub read_fasta {
	my ($fapathin,$checklabel,$order) = @_;
	my %fasta;
	my $id;
	my ($name,$dir,$suffix) = fileparse($fapathin);
	$name =~ s/^(.+)\.[^.]+$/$1/;
	my $barcodelabelsadded = 0;
	open my $fa_in, '<', $fapathin or die "Couldn't open $fapathin\n";
	my $n;
	while(my $row = <$fa_in>){
		chomp $row;
		if($row =~ /^>(.+)$/){
			$n++;
			$id = $1;
			if($checklabel and !($id =~ /;barcodelabel=[^;]+;/)){
				$barcodelabelsadded++;
				$id.=";barcodelabel=$name;";
			}
			$id =~ s/\s/_/g;
		} elsif($row =~ /^[ATCGNatcgn]+$/){
			if($order){
				$fasta{$id}{order} = $n;
				$fasta{$id}{seq} .= $row;
			} else {
				$fasta{$id} .= $row;
			}
		} else { die "Couldn't read fasta format in $fapathin\n" };
	}
	close $fa_in;
	print "File $fapathin missing barcode labels, \";barcodelabel=$name;\" appended to header of $barcodelabelsadded sequences\n" if $checklabel and $barcodelabelsadded and $verbose;
	return %fasta;
}

sub convert_uc_and_write {
	my ($ucpath,$outpath,$samples) = @_;
	my %tab;
	my %otus;
	open my $uc, '<', $ucpath or die "Error opening $ucpath\n";
	while(my $row = <$uc>){
		chomp $row;
		if($row =~ /^H.*barcodelabel=([^;]+);\t(.*)$/){
			$tab{$1}{$2}++;
			${$samples}{$1}++;
			$otus{$2}=1;
		}
	}
	close $uc;
	
	open my $csv, '>', $outpath or die "Error opening $outpath\n";
	print $csv "sample,",join(",",sort keys %otus),"\n";
	foreach my $sample (sort keys %$samples){
		print $csv $sample;
		if(${$samples}{$sample}>0){
			foreach my $otu (sort keys %otus){
				print $csv ",",$tab{$sample}{$otu} ? $tab{$sample}{$otu} : 0;
			}
		} else {
			print $csv ",0" x scalar keys %otus;
		}
		print $csv "\n";
	}
	close $csv;
}
